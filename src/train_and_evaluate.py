## Train the model
## Evaluate and track the metrics and parameters
## Save the model

import argparse
import json
import os
from get_data import GetData
from logger import AppLogger
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, plot_confusion_matrix, accuracy_score, f1_score
from sklearn.model_selection import train_test_split
import pandas as pd
import pickle
import numpy as np
import joblib
import mlflow
from urllib.parse import urlparse


class trainModel:
    """
    This class will train and save the model
    """
    def __init__(self):
        self.get_data = GetData()
        self.logger = AppLogger()
        self.file = open("Logs/ModelTraining.txt", "a")


    def eval_metircs(self, actual, y_pred, y_proba):
        """
        This method will evaluate the metrics of the model
        """
        try:

            accuracy = accuracy_score(actual, y_pred)
            f1 = f1_score(actual, y_pred)
            roc_auc = roc_auc_score(actual, y_proba)

            return accuracy, f1, roc_auc

        except Exception as e:
            raise e


    def train_and_evaluate(self, config_path):
        """
        This method will train and evaluate the model.
        """

        try:
            config = self.get_data.read_params(config_path)
            train_data_path = config["split_data"]["train_path"]
            test_data_path = config["split_data"]["test_path"]
            random_state = config["base"]["random_state"]
            model_dir = config["model_dir"]

            target_column = config["base"]["target_col"]

            n_estimators = config["estimators"]["RandomForestClassifier"]["params"]["n_estimators"]
            min_samples_split = config["estimators"]["RandomForestClassifier"]["params"]["min_samples_split"]
            min_samples_leaf = config["estimators"]["RandomForestClassifier"]["params"]["min_samples_leaf"]
            max_features = config["estimators"]["RandomForestClassifier"]["params"]["max_features"]
            max_depth = config["estimators"]["RandomForestClassifier"]["params"]["max_depth"]
            criterion = config["estimators"]["RandomForestClassifier"]["params"]["criterion"]

            train = pd.read_csv(train_data_path, sep=',')
            test = pd.read_csv(test_data_path, sep=',')
            self.logger.log(self.file, f"Successfully read the data")

            X_train = train.drop(columns=target_column)
            X_test = test.drop(columns=target_column)

            y_train = train[target_column]
            y_test = test[target_column]

############################# MLFLOW ################################################

            mlflow_config = config["mlflow_config"]
            experiment_name = mlflow_config["experiment_name"]
            run_name = mlflow_config["run_name"]
            registered_model_name = mlflow_config["registered_model_name"]
            remote_server_uri = mlflow_config["remote_server_uri"]

            mlflow.set_tracking_uri(remote_server_uri)

            # Setting the name of the experiment
            mlflow.set_experiment(experiment_name=experiment_name)

            with mlflow.start_run(run_name=run_name):

                clf = RandomForestClassifier(n_estimators=n_estimators, min_samples_split=min_samples_split,
                                       min_samples_leaf=min_samples_leaf, max_features=max_features, max_depth=max_depth,
                                             criterion=criterion)
                clf.fit(X_train, y_train)

                y_pred = clf.predict(X_test)
                y_proba = clf.predict_proba(X_test)[:,1]

                accuracy, f1, roc_auc = self.eval_metircs(y_test, y_pred, y_proba)
                self.logger.log(self.file, f"Successfully calculated the score: Accuracy: {accuracy}, F1: {f1}, AUC: {roc_auc}")

                print(f"RandomForest model: n_estimators={n_estimators}, min_samples_split={min_samples_split},"
                      f"min_samples_leaf= {min_samples_leaf}, max_features= {max_features}, max_depth= {max_depth},"
                      f"criterion= {criterion}")
                print(f"Accuracy: {accuracy}")
                print(f"F1: {f1}")
                print(f"AUC: {roc_auc}")

                mlflow.log_metric("Accuracy", accuracy)
                mlflow.log_metric("F1", f1)
                mlflow.log_metric("ROC_AUC", roc_auc)

                mlflow.log_param("n_estimators", n_estimators)
                mlflow.log_param("min_samples_split", min_samples_split)
                mlflow.log_param("min_samples_leaf", min_samples_leaf)
                mlflow.log_param("max_features", max_features)
                mlflow.log_param("max_depth", max_depth)
                mlflow.log_param("criterion", criterion)


                tracking_url_type_store = urlparse(mlflow.get_artifact_uri()).scheme

                if tracking_url_type_store != "file":
                    mlflow.sklearn.log_model(clf, "model", registered_model_name=registered_model_name)

                else:
                    mlflow.sklearn.log_model(clf, "model", registered_model_name=registered_model_name)


        except Exception as e:
            raise e


if __name__ == "__main__":
    args = argparse.ArgumentParser()
    args.add_argument("--config", default="params.yaml")
    parse_args = args.parse_args()
    train = trainModel()
    train.train_and_evaluate(config_path=parse_args.config)