## Train the mode
## Evaluate and track the metrics and parameters
## Save the model

import argparse
import json
import os
from get_data import GetData
from logger import AppLogger
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, plot_confusion_matrix, accuracy_score, f1_score
from sklearn.model_selection import train_test_split
import pandas as pd
import pickle
import numpy as np
import joblib

class trainModel:
    """
    This class will train and save the model
    """
    def __init__(self):
        self.get_data = GetData()
        self.logger = AppLogger()
        self.file = open("Logs/ModelTraining.txt", "a")


    def eval_metircs(self, actual, y_pred, y_proba):
        """
        This method will evaluate the metrics of the model
        """
        try:

            accuracy = accuracy_score(actual, y_pred)
            f1 = f1_score(actual, y_pred)
            roc_auc = roc_auc_score(actual, y_proba)

            return accuracy, f1, roc_auc

        except Exception as e:
            raise e


    def train_and_evaluate(self, config_path):
        """
        This method will train and evaluate the model.
        """

        try:
            config = self.get_data.read_params(config_path)
            train_data_path = config["split_data"]["train_path"]
            test_data_path = config["split_data"]["test_path"]
            random_state = config["base"]["random_state"]
            model_dir = config["model_dir"]

            target_column = config["base"]["target_col"]

            n_estimators = config["estimators"]["RandomForestClassifier"]["params"]["n_estimators"]
            min_samples_split = config["estimators"]["RandomForestClassifier"]["params"]["min_samples_split"]
            min_samples_leaf = config["estimators"]["RandomForestClassifier"]["params"]["min_samples_leaf"]
            max_features = config["estimators"]["RandomForestClassifier"]["params"]["max_features"]
            max_depth = config["estimators"]["RandomForestClassifier"]["params"]["max_depth"]
            criterion = config["estimators"]["RandomForestClassifier"]["params"]["criterion"]

            train = pd.read_csv(train_data_path, sep=',')
            test = pd.read_csv(test_data_path, sep=',')
            self.logger.log(self.file, f"Successfully read the data")

            X_train = train.drop(columns=target_column)
            X_test = test.drop(columns=target_column)

            y_train = train[target_column]
            y_test = test[target_column]

            # Training the model
            clf = RandomForestClassifier(n_estimators=n_estimators, min_samples_split=min_samples_split,
                                   min_samples_leaf=min_samples_leaf, max_features=max_features, max_depth=max_depth,
                                         criterion=criterion)
            clf.fit(X_train, y_train)

            y_pred = clf.predict(X_test)
            y_proba = clf.predict_proba(X_test)[:,1]

            accuracy, f1, roc_auc = self.eval_metircs(y_test, y_pred, y_proba)
            self.logger.log(self.file, f"Successfully calculated the score: Accuracy: {accuracy}, F1: {f1}, AUC: {roc_auc}")

            print(f"RandomForest model: n_estimators={n_estimators}, min_samples_split={min_samples_split},"
                  f"min_samples_leaf= {min_samples_leaf}, max_features= {max_features}, max_depth= {max_depth},"
                  f"criterion= {criterion}")
            print(f"Accuracy: {accuracy}")
            print(f"F1: {f1}")
            print(f"AUC: {roc_auc}")

########################################################################
            # Tracking the model
            scores_file = config["reports"]["scores"]
            params_file = config["reports"]["params"]

            with open(scores_file, "a") as f:
                scores = {
                    "Accuracy": accuracy,
                    "F1": f1,
                    "ROC_AUC": roc_auc
                }
                json.dump(scores, f, indent=4)
            self.logger.log(self.file, "Scores logged successfully")

            with open(params_file, "a") as f:
                params = {
                    "n_estimators": n_estimators,
                    "min_samples_split": min_samples_split,
                    "min_samples_leaf": min_samples_leaf,
                    "max_features": max_features,
                    "max_depth": max_depth,
                    "criterion": criterion
                }
                json.dump(params, f, indent=4)
            self.logger.log(self.file, "Parameters logged successfully")

##################################################################################
            # Saving the model
            os.makedirs(model_dir, exist_ok=True)
            model_path = os.path.join(model_dir, "model.joblib")
            joblib.dump(clf, model_path)
            # model_path = open(os.path.join(model_dir, "model.pkl"), "wb")
            # pickle.dump(clf, model_path)
            self.logger.log(self.file, "Model saved successfully")
            self.file.close()

        except Exception as e:
            self.logger.log(self.file, f"Error occurred {e}")
            self.file.close()
            raise e



if __name__ == "__main__":
    args = argparse.ArgumentParser()
    args.add_argument("--config", default="params.yaml")
    parse_args = args.parse_args()
    train = trainModel()
    train.train_and_evaluate(config_path=parse_args.config)
